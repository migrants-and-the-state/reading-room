<script>
	import { base } from '$app/paths';
	import { Breadcrumb, BreadcrumbItem, Link, Tag } from 'carbon-components-svelte';

	const ms_provenance_tags = {
		ms_ocr_v1:
			'OCR (Optical Character Recognition) processes images of original A-File hard-copy scans by recognizing text in the images of documents and making it machine readable. Once characters are recognized as such, it becomes key-word searchable and the machine-learning models are then able to read, contextualize, and perform other tasks in relation to the text.',
		ms_doctype_v1:
			'Document types are derrived through use of a Convolutional Neural Network (CNN), a type of artificial intelligence that helps computers understand images by breaking them into small parts to find patterns like shapes, colors, or edges, this model identifies the difference between photos, long-form text documents like letters or affidavits, and government forms with repeated structured data and patters.',
		ms_sex_llm_v1:
			'This data is extracted by the LLM from the individual pages of A-Files, typically from forms where a biological sex designation (e.g. Male / Female) is required. The LLM then aggregates the total number of identifications for biological sex and predicts the sex of the A-File holder.',
		ms_form_title_llm_v1:
			'A Large Language Model (LLM), a type of artificial intelligence that understands text and language, analyzes any image identified as a form by the CNN model and extracts the form number and title which normally appear in the same locations across the various government form types. The field is then populated with a list of all of the form titles (which typically appear along with the form number) that appear in the file.',
		ms_countries_nlp_v1:
			'To extract country names from an image, the (OCR) software first converts the text in the images into machine-readable text. NLP then analyzes the extracted text to understand its structure and meaning, identifying patterns and extracting specific information like names or locations. Named Entity Recognition (NER), is a component of NLP trained on large datasets of country names and related linguistic features. NER uses context clues, grammar, and patterns to distinguish country names from other text by recognizing patterns like capitalization, proximity to keywords (e.g., "located in").  Once country names have been identified, they are compiled in thies field as a full list of all the country names identified on the page.',
		ms_years_nlp_v1:
			'The NLP model examines and parses text using Named Entity Recognition (NER) that relies on pattern identification (e.g. grammar, context, and repeated formulations like Date of Birth:____ or Date of Entry:____). The NLP identifies and extracts every instance where a date with a year appears on a page. The years are then compiled into this field as a list of all the years recorded on the page.',
		nationality_llm_v1: '',
		reason_llm_v1: '',
		occupation_llm_v1:
			'After the CNN model identies the an image as a form, the LLM extracts the form number and title from the OCR text. If the LLM identifies a form G325, it is then trained to examine that form spacially, and extract the occupation data listed by the A-File holder in a specific location on the form.',
		complexion_llm_v1:
			'After the CNN model identies the an image as a form, the LLM extracts the form number and title from the OCR text. If the LLM identifies a form Certificate of Naturalization, it is then trained to examine that form spacially, and extract the biometric complexion data which is consistently located in a certain spot on the form.',
		marital_status_llm_v1:
			'After the CNN model identies the an image as a form, the LLM extracts the form number and title from the OCR text. If the LLM identifies a form Certificate of Naturalization, it is then trained to examine that form spacially, and extract the mertial status data which is consistently located in a certain spot on the form.',
		residence_naturlization_llm_v1:
			'After the CNN model identies the an image as a form, the LLM extracts the form number and title from the OCR text. If the LLM identifies a form Certificate of Naturalization, it is then trained to examine that form spacially, and extract the residence at naturalization data which is consistently located in a certain spot on the form.',
		year_naturalization_llm_v1:
			'After the CNN model identies the an image as a form, the LLM extracts the form number and title from the OCR text. If the LLM identifies a form Certificate of Naturalization, it is then trained to examine that form spacially, and extract the date of naturalization data which is consistently located in a certain spot on the form.'
	};
</script>

<Breadcrumb noTrailingSlash class="mb-8">
	<BreadcrumbItem href={base}>M/S Corpus UI v1-Sample</BreadcrumbItem>
	<BreadcrumbItem href="/data-guide" isCurrentPage>Data Guide</BreadcrumbItem>
</Breadcrumb>

<div class="max-w-[85ch]">
	<h1 class="mb-8">Data Guide</h1>

	<h2 class="my-4">Introduction</h2>

	<h3 class="my-2 text-xl font-semibold">Metadata Sources and Training Set</h3>
	<p class="py-2">
		This prototype allows users to search the test set of A-files using metadata derived from two
		sources: the publicly available U.S. National Archives (NARA) catalog metadata and metadata
		based on information extracted by our machine learning models.
	</p>
	<p class="py-2">
		The metadata available in the NARA catalog for A-files varies; the process of transferring files
		that have entered the public domain from the federal agency governing immigration (INS or USCIS)
		to NARA takes place every five years. At the time of each of these transfers, the agencies
		determine which metadata fields attached to the files will be transferred, resulting in
		variability in metadata across the entire NARA catalog (approximately 1.6 million A-files). The
		basic metadata always available from the NARA catalog for each A-file includes date of birth,
		first name, last name, and country of birth. Additional metadata from the NARA catalog can
		include sex, date of entry, port of entry, date file opened, father’s name, mother’s name, date
		and location of naturalization, naturalization certificate number, and class of admission.
		Reference tables are available to identify the agency codes used for countries, ports of entry,
		and naturalization locations.
	</p>
	<p class="py-2">
		The metadata created by the Migrants and the State project has been derived using a variety of
		machine learning methods deployed on a training set of XXX files/XXX pages of A-files. Most of
		the training set is made up of files ordered from the Kansas City regional office of NARA, where
		the vast majority of public domain A-files are held. While these A-files are in the public
		domain, NARA reviews each file for third-party privacy violations and redacts information as
		necessary. NARA levies a processing charge of either $27 or $40 (depending on the file holder’s
		year of birth) for each A-file ordered online. Faced with a catalog of a million files and a
		budget for 550 files, we made selections designed to achieve regional and temporal breadth, with
		additional files for two countries of origin (Mexico and Haiti) to enable greater depth of
		research. The remainder of the training set comes from A-files made public by NARA’s San
		Francisco regional office and a relatively small number of A-files obtained from USCIS via that
		agency’s Electronic Reading Room and our Freedom of Information Act (FOIA) requests for the
		A-files of verifiably deceased individuals. The USCIS files are being used for model training
		purposes only and are not included in this prototype.
	</p>

	<h3 class="my-2 text-xl font-semibold">Model Training</h3>
	<p class="py-2">
		The model training work has involved two main stages: categorizing documents and extracting
		information from them. In general, our approach has prioritized training models to address a
		larger number of data identification tasks rather than attempting fewer tasks and refining the
		models to achieve peak accuracy. We started by developing a model (using a convolutional neural
		network approach) to create image embeddings for every page in the training set. Members of the
		team reviewed each page to identify machine-generated errors and used that information to
		improve the model’s performance to at least 85% accuracy in distinguishing among forms, letters,
		photographs, and other kinds of materials (which we labelled “miscellaneous”).
	</p>
	<p class="py-2">
		While the model identifying document types relied on the visual information on each A-file page,
		the ability to extract text information required the use of optical character recognition (OCR)
		to create a machine readable text version of the training set. This text allows us to perform
		general text extraction (identifying the presence of text in the body of a document, similar to
		a keyword search) as well as extracting information from a specific location within a document
		(for example, a form title or answer to a question). We used existing Named Entity Recognition
		(NER) techniques to extract information about the years and country names included in each
		A-file. For document-specific metadata extraction, we chose the G-325 and Certificate of
		Naturalization forms as our test cases for developing models (using a large language model
		approach) that can identify a form type and extract specific information from it. The
		information the models can extract from these forms (employment history, residential addresses,
		and biographical details) is valuable in its own right, but we are also attempting to develop a
		scalable technique that can later be applied to other forms.
	</p>

	<h3 class="my-2 text-xl font-semibold">Metadata on Sex and Race</h3>
	<p class="py-2">
		The NARA catalogue metadata, compiled as a limited subset of the metadata created by INS /
		USCIS, frequently lacks entries corresponding to the field designating biological sex. But
		information on biological sex, along with other biometric data required by government forms,
		appears frequently across documents in A-Files. We created a model to extract this data from the
		individual pages of A-Files, typically from forms where a biological sex designation (e.g. Male
		/ Female) is required. The model then aggregates the total number of identifications for
		biological sex and predicts the sex of the A-File holder.
	</p>
	<p class="py-2">
		Extracting metadata to reflect historical government categories and bureaucratic norms around
		biological sex is relatively straightforward, given the binary categories then in use of male
		and female. Extracting metadata about racial categorization is more complex. This information
		often takes the form of indications of a file holder’s complexion, for which migrants and
		immigration officers used a variety of adjectives, many of them offensive to contemporary
		sensibilities. While our models extract that information from the G325 and Certificate of
		Naturalization forms, we have not yet created a controlled vocabulary or other way of searching
		for it. We look forward to receiving feedback about how best to handle this issue.
	</p>
</div>

<h2 class="py-4">Methods Glossary</h2>
<div class="my-10 flex flex-wrap gap-8">
	<dl class="basis-1/4">
		<dt class="font-semibold">Large Language Model (LLM)</dt>
		<dd class="py-2">Lorem ipsum</dd>
	</dl>
	<dl class="basis-1/4">
		<dt class="font-semibold">Machine Learning (ML)</dt>
		<dd class="py-2">Lorem ipsum</dd>
	</dl>
	<dl class="basis-1/4">
		<dt class="font-semibold">Natural Language Processing (NLP)</dt>
		<dd class="py-2">Lorem ipsum</dd>
	</dl>
	<dl class="basis-1/4">
		<dt class="font-semibold">Named Entity Recognition (NER)</dt>
		<dd class="py-2">Lorem ipsum</dd>
	</dl>
	<dl class="basis-1/4">
		<dt class="font-semibold">Convolutional Neural Network (CNN)</dt>
		<dd class="py-2">Lorem ipsum</dd>
	</dl>
	<dl class="basis-1/4">
		<dt class="font-semibold">Optical Character Recognition (OCR)</dt>
		<dd class="py-2">Lorem ipsum</dd>
	</dl>
</div>

<h2 class="py-4">Data Provenance Tags</h2>
<div class="w-full">
	<div class="flex gap-6 py-4">
		<div class="min-w-56">
			<Link href="#nara">
				<Tag interactive id="nara" type="blue">#nara</Tag>
			</Link>
		</div>
		<div>
			<p class="max-w-[85ch] text-sm">
				Fields with this tag come directly from the U.S. National Archives catalog.
			</p>
		</div>
	</div>

	{#each Object.entries(ms_provenance_tags) as [tag, description]}
		<div class="flex gap-6 py-4">
			<div class="min-w-56">
				<Link href={`#${tag}`}>
					<Tag interactive id={tag} type="green">#{tag}</Tag>
				</Link>
			</div>
			<div>
				<p class="max-w-[85ch] text-sm">
					{description}
				</p>
			</div>
		</div>
	{/each}
</div>
